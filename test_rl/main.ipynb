{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Robert\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gym_chess import ChessEnvV1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           >>>>>>>>>> WHITE\n",
      "    -------------------------\n",
      " 8 |  ♖  ♘  ♗  ♕  ♔  ♗  ♘  ♖ |\n",
      " 7 |  ♙  ♙  ♙  ♙  ♙  ♙  ♙  ♙ |\n",
      " 6 |  .  .  .  .  .  .  .  . |\n",
      " 5 |  .  .  .  .  .  .  .  . |\n",
      " 4 |  .  .  .  .  .  .  .  . |\n",
      " 3 | \u001b[42m\u001b[30m . \u001b[0m\u001b[0m .  .  .  .  .  .  . |\n",
      " 2 | \u001b[47m\u001b[30m ♟ \u001b[0m\u001b[0m ♟  ♟  ♟  ♟  ♟  ♟  ♟ |\n",
      " 1 |  ♜  ♞  ♝  ♛  ♚  ♝  ♞  ♜ |\n",
      "    -------------------------\n",
      "      a  b  c  d  e  f  g  h \n",
      "           >>>>>>>>>> BLACK\n",
      "    -------------------------\n",
      " 8 |  ♖  ♘  ♗  ♕  ♔  ♗ \u001b[47m\u001b[30m ♘ \u001b[0m\u001b[0m ♖ |\n",
      " 7 |  ♙  ♙  ♙  ♙  ♙  ♙  ♙  ♙ |\n",
      " 6 |  .  .  .  .  . \u001b[42m\u001b[30m . \u001b[0m\u001b[0m .  . |\n",
      " 5 |  .  .  .  .  .  .  .  . |\n",
      " 4 |  .  .  .  .  .  .  .  . |\n",
      " 3 |  ♟  .  .  .  .  .  .  . |\n",
      " 2 |  .  ♟  ♟  ♟  ♟  ♟  ♟  ♟ |\n",
      " 1 |  ♜  ♞  ♝  ♛  ♚  ♝  ♞  ♜ |\n",
      "    -------------------------\n",
      "      a  b  c  d  e  f  g  h \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gym_chess import ChessEnvV1\n",
    "\n",
    "env = ChessEnvV1() # or ChessEnvV2\n",
    "\n",
    "observation_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "# current state\n",
    "state = env.state\n",
    "\n",
    "# select a move and convert it into an action\n",
    "moves = env.possible_moves\n",
    "move = random.choice(moves)\n",
    "# action = env.move_to_actions(move)\n",
    "\n",
    "# or select an action directly\n",
    "actions = env.possible_actions\n",
    "action = random.choice(actions)\n",
    "\n",
    "# pass it to the env and get the next state\n",
    "new_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3, -5, -4, -2, -1, -4, -5, -3],\n",
       "       [-6, -6, -6, -6, -6, -6, -6, -6],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 3,  5,  4,  2,  1,  4,  5,  3]], dtype=int8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -------------------------\n",
      " 8 |  ♖  ♘  ♗  ♕  ♔  ♗  ♘  ♖ |\n",
      " 7 |  ♙  ♙  ♙  ♙  ♙  ♙  ♙  ♙ |\n",
      " 6 |  .  .  .  .  .  .  .  . |\n",
      " 5 |  .  .  .  .  .  .  .  . |\n",
      " 4 |  .  .  .  .  .  .  .  . |\n",
      " 3 |  .  .  .  .  .  .  .  . |\n",
      " 2 |  ♟  ♟  ♟  ♟  ♟  ♟  ♟  ♟ |\n",
      " 1 |  ♜  ♞  ♝  ♛  ♚  ♝  ♞  ♜ |\n",
      "    -------------------------\n",
      "      a  b  c  d  e  f  g  h \n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -------------------------\n",
      " 8 |  ♖  ♘  ♗  ♕  ♔  ♗  ♘  ♖ |\n",
      " 7 |  ♙  ♙  ♙  ♙  ♙  ♙  ♙  ♙ |\n",
      " 6 |  .  .  .  .  .  .  .  . |\n",
      " 5 |  .  .  .  .  .  .  .  . |\n",
      " 4 |  .  .  .  .  . \u001b[42m\u001b[30m . \u001b[0m\u001b[0m .  . |\n",
      " 3 | \u001b[42m\u001b[30m . \u001b[0m\u001b[0m . \u001b[42m\u001b[30m . \u001b[0m\u001b[0m .  . \u001b[42m\u001b[30m . \u001b[0m\u001b[0m .  . |\n",
      " 2 |  ♟  ♟  ♟  ♟  ♟ \u001b[47m\u001b[30m ♟ \u001b[0m\u001b[0m ♟  ♟ |\n",
      " 1 |  ♜ \u001b[47m\u001b[30m ♞ \u001b[0m\u001b[0m ♝  ♛  ♚  ♝  ♞  ♜ |\n",
      "    -------------------------\n",
      "      a  b  c  d  e  f  g  h \n"
     ]
    }
   ],
   "source": [
    "moves = env.possible_moves\n",
    "env.render_moves(moves[10:12] + moves[16:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, env):\n",
    "        self._max_action = self._action_space_low = env.action_space.low\n",
    "        self._action_space_high = env.action_space.high\n",
    "        self._act_dim = act_dim\n",
    "        self.policy_name = \"RandomPolicy\"\n",
    "def get_action(self, state):\n",
    "        return np.random.uniform(\n",
    "            low=-self._max_action,\n",
    "            high=self._max_action,\n",
    "            size=self._act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'low'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-93f084ffc627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# play 4 games\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnumber_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnumber_moves\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-ca2f302a0289>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mRandomPolicy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_space_low\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_space_high\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_act_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Discrete' object has no attribute 'low'"
     ]
    }
   ],
   "source": [
    "policy = RandomPolicy(env)\n",
    "# play 4 games\n",
    "number_episodes = 4\n",
    "number_moves    = 100\n",
    "for i in range(number_episodes):\n",
    "    # initialize the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    game_rew = 0  # accumulated reward\n",
    "    for j in range(number_moves):\n",
    "        # choose a random action\n",
    "        action = policy.get_action(state)\n",
    "        # take a step in the environment\n",
    "        next_state, rew, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        game_rew += rew\n",
    "        env.render()\n",
    "        # when is done, print the cumulative reward of the game and reset the environment\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "    print('Episode %d finished, reward:%d, the lenght of the episode:%d'% (i, game_rew,j))\n",
    "# close the redering window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "class PolicyGradientNNAgent():\n",
    "\n",
    "  def __init__(self,\n",
    "    lr=0.5, \n",
    "    gamma=0.99, \n",
    "    lam=0.002,\n",
    "    state_size=4,\n",
    "    action_size=2,\n",
    "    n_hidden_1=20,\n",
    "    n_hidden_2=20,\n",
    "    scope=\"pg\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    args\n",
    "      epsilon           exploration rate\n",
    "      epsilon_anneal    linear decay rate per call of learn() function (iteration)\n",
    "      end_epsilon       lowest exploration rate\n",
    "      lr                learning rate\n",
    "      gamma             discount factor\n",
    "      state_size        network input size\n",
    "      action_size       network output size\n",
    "    \"\"\"\n",
    "    self.lr = lr\n",
    "    self.lam = lam\n",
    "    self.gamma = gamma\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.total_steps = 0\n",
    "    self.n_hidden_1 = n_hidden_1\n",
    "    self.n_hidden_2 = n_hidden_2\n",
    "    self.scope = scope\n",
    "\n",
    "\n",
    "    self.state_input = tf.placeholder(tf.float32, [None, self.state_size])\n",
    "    self.action = tf.placeholder(tf.int32, [None])\n",
    "    self.target = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    self.create_policy(self.state_input)\n",
    "    self.create_training(self.action, self.target)\n",
    "\n",
    "\n",
    "  def create_policy(self, state_input):\n",
    "    \"\"\"\n",
    "    Create the policy network.\n",
    "    \"\"\"\n",
    "    layer_1 = tf.layers.dense(state_input, self.n_hidden_1, activation=tf.nn.relu)\n",
    "    layer_2 = tf.layers.dense(layer_1, self.n_hidden_2, activation=tf.nn.relu)\n",
    "\n",
    "    self.action_values = tf.layers.dense(layer_2, self.action_size)\n",
    "    self.action_prob = tf.nn.softmax(self.action_values)\n",
    "\n",
    "  def create_training(self, action_taken, target_value_function):\n",
    "    \"\"\"\n",
    "    Create the training function.\n",
    "    \"\"\"\n",
    "    action_mask = tf.one_hot(action_taken, self.action_size, 1.0, 0.0)\n",
    "    self.action_value_pred = tf.reduce_sum(self.action_prob * action_mask, 1)\n",
    "\n",
    "    self.l2_loss = tf.add_n([ tf.nn.l2_loss(v) for v in tf.trainable_variables()  ]) \n",
    "\n",
    "    # POLICY GRADIENT LOSS!\n",
    "    self.pg_loss = tf.reduce_mean(-tf.log(self.action_value_pred) * target_value_function)\n",
    "\n",
    "    self.loss = self.pg_loss + self.lam * self.l2_loss\n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "    self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "\n",
    "  def get_action(self, state, sess):\n",
    "    \"\"\"\n",
    "    Randomly sample the policy.\n",
    "    \"\"\"\n",
    "    pi = self.get_policy(state, sess)\n",
    "    random_discrte_action = np.random.choice(list(range(self.action_size)), p=pi)\n",
    "    return random_discrte_action\n",
    "\n",
    "\n",
    "  def get_policy(self, state, sess):\n",
    "    \"\"\"returns policy as probability distribution of actions\"\"\"\n",
    "    pi = sess.run(self.action_prob, feed_dict={self.state_input: [state]})\n",
    "    return pi[0]\n",
    "\n",
    "\n",
    "  def learn(self, episode, sess,):\n",
    "    \"\"\"\n",
    "    The training loop for a single episode.\n",
    "    Args:\n",
    "      episode: A list of (State, action, next_state, reward, done) pairs\n",
    "      sess: The Tensorflow session.\n",
    "    \"\"\"\n",
    "    for t in range(len(episode)):\n",
    "      self.total_steps = self.total_steps + 1\n",
    "      target = sum([self.gamma**i * r for i, (s, a, s1, r, d) in enumerate(episode[t:])])\n",
    "      state, action, next_state, reward, done = episode[t]\n",
    "      feed_dict = { self.state_input: [state], self.target: [target], self.action: [action] }\n",
    "      sess.run([self.train_op, self.loss], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-bcfdc9302a92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m agent = PolicyGradientNNAgent(lr=LEARNING_RATE,\n\u001b[0m\u001b[0;32m     64\u001b[0m                                           \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                           \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-03459c0dae0a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lr, gamma, lam, state_size, action_size, n_hidden_1, n_hidden_2, scope)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "NUM_EPISODES = 800\n",
    "MAX_STEPS = 200\n",
    "FAIL_PENALTY = -100\n",
    "# LEARNING_RATE = 0.0001 # hidden layer 10/20\n",
    "LEARNING_RATE = 0.002 # hidden layer 5\n",
    "# LEARNING_RATE = 0.1 # hidden layer 3\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "TRAIN_EVERY_NUM_EPISODES = 1\n",
    "EPOCH_SIZE = 1\n",
    "MEM_SIZE = 100\n",
    "\n",
    "RECORD = False\n",
    "\n",
    "\n",
    "def train(agent, env, sess, num_episodes=NUM_EPISODES):\n",
    "  history = []\n",
    "  for i in range(NUM_EPISODES):\n",
    "    if i % 20 == 0:\n",
    "      print(\"Episode {}\".format(i + 1))\n",
    "      print(\"Doing evaluation.\")\n",
    "      tot_reward = 0\n",
    "      cur_state = env.reset()\n",
    "      for t in range(MAX_STEPS):\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        action = agent.get_action(cur_state, sess)\n",
    "        cur_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "          print(\"\\tAgent lasted for {}\".format(t))\n",
    "          break\n",
    "    cur_state = env.reset()\n",
    "    episode = []\n",
    "    for t in range(MAX_STEPS):\n",
    "      action = agent.get_action(cur_state, sess)\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        reward = FAIL_PENALTY\n",
    "        episode.append([cur_state, action, next_state, reward, done])\n",
    "        # if i % 10 == 0:\n",
    "        # print((\"Episode finished after {} timesteps\".format(t + 1)))\n",
    "        # print(agent.get_policy(cur_state, sess))\n",
    "        history.append(t + 1)\n",
    "        break\n",
    "      episode.append([cur_state, action, next_state, 1, done])\n",
    "      cur_state = next_state\n",
    "      if t == MAX_STEPS - 1:\n",
    "        history.append(t + 1)\n",
    "        print((\"Episode finished after {} timesteps\".format(t + 1)))\n",
    "    # agent.add_episode(episode)\n",
    "    if i % TRAIN_EVERY_NUM_EPISODES == 0:\n",
    "      # print('train at episode {}'.format(i))\n",
    "      agent.learn(episode, sess)\n",
    "  return agent, history\n",
    "\n",
    "\n",
    "agent = PolicyGradientNNAgent(lr=LEARNING_RATE,\n",
    "                                          gamma=DISCOUNT_FACTOR,\n",
    "                                          state_size=4,\n",
    "                                          action_size=2,\n",
    "                                          n_hidden_1=5,\n",
    "                                          n_hidden_2=5)\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 200\n",
    "if RECORD:\n",
    "  env = wrappers.Monitor(env, '/tmp/cartpole-experiment-2', force=True)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  agent, history = train(agent, env, sess)\n",
    "\n",
    "\n",
    "if RECORD:\n",
    "  env.monitor.close()\n",
    "\n",
    "window = 10\n",
    "avg_reward = [numpy.mean(history[i*window:(i+1)*window]) for i in range(int(len(history)/window))]\n",
    "f_reward = plt.figure(1)\n",
    "plt.plot(numpy.linspace(0, len(history), len(avg_reward)), avg_reward)\n",
    "plt.ylabel('Rewards')\n",
    "f_reward.show()\n",
    "print('press enter to continue')\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6a8ea6a14b610647d44a342163a38c2c8a38bc76032faff98da0ea72fd517de"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
